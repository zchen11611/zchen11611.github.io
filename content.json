{"meta":{"title":null,"subtitle":"","description":"基于GoodHexo便携包搭建的博客，官方网站：http://yiwangmeng.com","author":"Zhi Chen","url":"https://zchen11611.github.io","root":"/"},"pages":[],"posts":[{"title":"WELCOME TO My Page","slug":"index","date":"2020-12-14T08:25:48.114Z","updated":"2020-12-14T08:25:48.114Z","comments":true,"path":"index.html","link":"","permalink":"https://zchen11611.github.io/index.html","excerpt":"","text":"Hi, I’m Zhi Chen. This webpage I will share my weekly reports, course project and any other contributions. All can be accessed using the top navigation menu: The weekly reports page will be about my weekly work on the Internet of Things course (EE-629) and I will record the content of my weekly study. The GitHub page will have a link to access my repository. My project will be an ios demo based on object-c(swift) called Face recognition_live detection. Labs: could not get my hand on a Raspberry Pi, therefore, I tried to study it manually which helped me understand how to handle it, and hopefully, in coming days I will buy one as I have plans to implement some ideas with it. Project: can be found on Project page with the code.","categories":[],"tags":[]},{"title":"Final Project- Face recognition_live detection","slug":"Project","date":"2020-12-13T13:19:00.000Z","updated":"2020-12-13T13:19:06.000Z","comments":true,"path":"Project.html","link":"","permalink":"https://zchen11611.github.io/Project.html","excerpt":"","text":"Final Project- Face recognition_live detection 1.IntroductionThe well-known Alipay (Chinese payment platform) uses face++ services to realize face recognition. In actual projects, iFLYTEK’s face recognition SDK is used for secondary packaging to achieve live recognition.In actual use, many apps have implemented fingerprint login, gesture login, third-party login, and face-swiping login in order to ensure the safety of users. In addition to the regular account and password login, I will Share how the project realizes the live detection of face recognition, which is the most basic implementation for realizing face recognition login. 2.Project realization ideas Click the recognition button to call the camera CameraRules class, detect camera permissions Initialize page, create camera page, create mouth data and head shaking data Turn on recognition, face frame recognition Face recognition, face recognition determines whether a face is detected After detecting the face, determine the location The position is judged right, judge whether to open the mouth Open your mouth to judge, verify whether you shake your head Shake your head to judge the completion, 3 seconds countdown to take pictures After taking the picture, choose to retake or upload the picture Select retake and repeat steps 5-9, select upload to recall the image data Data clean 3.Source codeClick the recognition button to call the camera 1234567891011if([CameraRules isCapturePermissionGranted])&#123; [self setDeviceAuthorized:YES]; &#125; else&#123; dispatch_async(dispatch_get_main_queue(), ^&#123; NSString* info=@&quot;No camera permission&quot; [self showAlert:info]; [self setDeviceAuthorized:NO]; &#125;); &#125; CameraRules class, detect camera permissions 12345678910111213141516171819202122232425//Detect camera permissions+(BOOL)isCapturePermissionGranted&#123; if([AVCaptureDevice respondsToSelector:@selector(authorizationStatusForMediaType:)])&#123; AVAuthorizationStatus authStatus = [AVCaptureDevice authorizationStatusForMediaType:AVMediaTypeVideo]; if(authStatus ==AVAuthorizationStatusRestricted || authStatus ==AVAuthorizationStatusDenied)&#123; return NO; &#125; else if(authStatus==AVAuthorizationStatusNotDetermined)&#123; dispatch_semaphore_t sema = dispatch_semaphore_create(0); __block BOOL isGranted=YES; [AVCaptureDevice requestAccessForMediaType:AVMediaTypeVideo completionHandler:^(BOOL granted) &#123; isGranted=granted; dispatch_semaphore_signal(sema); &#125;]; dispatch_semaphore_wait(sema, DISPATCH_TIME_FOREVER); return isGranted; &#125; else&#123; return YES; &#125; &#125; else&#123; return YES; &#125;&#125; Initialize page, create camera page, create mouth data and head shaking data 1234//Create camera page, create mouth opening data and head shaking data [self faceUI]; [self faceCamera]; [self faceNumber]; Turn on recognition, face frame recognition 1234567891011121314151617181920float cx = (left+right)/2;float cy = (top + bottom)/2;float w = right - left;float h = bottom - top;float ncx = cy ;float ncy = cx ;CGRect rectFace = CGRectMake(ncx-w/2 ,ncy-w/2 , w, h);if(!isFrontCamera)&#123; rectFace=rSwap(rectFace); rectFace=rRotate90(rectFace, faceImg.height, faceImg.width);&#125;BOOL isNotLocation = [self identifyYourFaceLeft:left right:right top:top bottom:bottom];if (isNotLocation==YES) &#123; return nil;&#125; Face recognition, face recognition determines whether a face is detected 12345678910111213141516171819for(id key in keys)&#123; id attr=[landmarkDic objectForKey:key]; if(attr &amp;&amp; [attr isKindOfClass:[NSDictionary class]])&#123; if(!isFrontCamera)&#123; p=pSwap(p); p=pRotate90(p, faceImg.height, faceImg.width); &#125; if (isCrossBorder == YES) &#123; [self delateNumber]; return nil; &#125; p=pScale(p, widthScaleBy, heightScaleBy); [arrStrPoints addObject:NSStringFromCGPoint(p)]; &#125;&#125; After detecting the face, determine the location 1234567891011121314151617181920212223242526272829303132333435if (right - left &lt; 230 || bottom - top &lt; 250) &#123; self.textLabel.text = @&quot;too far&quot;; [self delateNumber]; isCrossBorder = YES; return YES;&#125;else if (right - left &gt; 320 || bottom - top &gt; 320) &#123; self.textLabel.text = @&quot;too close&quot;; [self delateNumber]; isCrossBorder = YES; return YES;&#125;else&#123; if (isJudgeMouth != YES) &#123; self.textLabel.text = @&quot;Please repeat opening mouth&quot;; [self tomAnimationWithName:@&quot;openMouth&quot; count:2]; if (left &lt; 100 || top &lt; 100 || right &gt; 460 || bottom &gt; 400) &#123; isCrossBorder = YES; isJudgeMouth = NO; self.textLabel.text = @&quot;Please adjust the position first&quot;; [self delateNumber]; return YES; &#125; &#125;else if (isJudgeMouth == YES &amp;&amp; isShakeHead != YES) &#123; self.textLabel.text = @&quot;Please repeat shaking your head&quot;; [self tomAnimationWithName:@&quot;shakeHead&quot; count:4]; number = 0; &#125;else&#123; takePhotoNumber += 1; if (takePhotoNumber == 2) &#123; [self timeBegin]; &#125; &#125; isCrossBorder = NO;&#125; The position is judged right, judge whether to open the mouth 12345678910111213141516171819202122232425if (rightX &amp;&amp; leftX &amp;&amp; upperY &amp;&amp; lowerY &amp;&amp; isJudgeMouth != YES) &#123; number ++; if (number == 1 || number == 300 || number == 600 || number ==900) &#123; mouthWidthF = rightX - leftX &lt; 0 ? abs(rightX - leftX) : rightX - leftX; mouthHeightF = lowerY - upperY &lt; 0 ? abs(lowerY - upperY) : lowerY - upperY; NSLog(@&quot;%d,%d&quot;,mouthWidthF,mouthHeightF); &#125;else if (number &gt; 1200) &#123; [self delateNumber]; [self tomAnimationWithName:@&quot;openMouth&quot; count:2]; &#125; mouthWidth = rightX - leftX &lt; 0 ? abs(rightX - leftX) : rightX - leftX; mouthHeight = lowerY - upperY &lt; 0 ? abs(lowerY - upperY) : lowerY - upperY; NSLog(@&quot;%d,%d&quot;,mouthWidth,mouthHeight); NSLog(@&quot;Before opening your mouth：width=%d，height=%d&quot;,mouthWidthF - mouthWidth,mouthHeight - mouthHeightF); if (mouthWidth &amp;&amp; mouthWidthF) &#123; if (mouthHeight - mouthHeightF &gt;= 20 &amp;&amp; mouthWidthF - mouthWidth &gt;= 15) &#123; isJudgeMouth = YES; imgView.animationImages = nil; &#125; &#125;&#125; Open your mouth to judge, verify whether you shake your head 123456789101112131415161718if ([key isEqualToString:@&quot;mouth_middle&quot;] &amp;&amp; isJudgeMouth == YES) &#123; if (bigNumber == 0 ) &#123; firstNumber = p.x; bigNumber = p.x; smallNumber = p.x; &#125;else if (p.x &gt; bigNumber) &#123; bigNumber = p.x; &#125;else if (p.x &lt; smallNumber) &#123; smallNumber = p.x; &#125; if (bigNumber - smallNumber &gt; 60) &#123; isShakeHead = YES; [self delateNumber]; &#125; &#125; Shake your head to judge the completion, 3 seconds countdown to take pictures 123456789101112if(timeCount &gt;= 1) &#123; self.textLabel.text = [NSString stringWithFormat:@&quot;%ld s后拍照&quot;,(long)timeCount]; &#125; else &#123; [theTimer invalidate]; theTimer=nil; [self didClickTakePhoto]; &#125; After taking the picture, choose to retake or upload the picture 1234567891011121314-(void)didClickPhotoAgain&#123; [self delateNumber]; [self.previewLayer.session startRunning]; self.textLabel.text = @&quot;Pls adjust your position&quot;; [backView removeFromSuperview]; isJudgeMouth = NO; isShakeHead = NO; &#125; Select retake and repeat steps 5-9, select upload to recall the image data 1234567-(void)didClickUpPhoto&#123; //successfully upload theimage [self.faceDelegate sendFaceImage:imageView.image]; [self.navigationController popViewControllerAnimated:YES];&#125; Data clean 123456789101112131415161718-(void)delateNumber&#123; number = 0; takePhotoNumber = 0; mouthWidthF = 0; mouthHeightF = 0; mouthWidth = 0; mouthHeight = 0; smallNumber = 0; bigNumber = 0; firstNumber = 0; imgView.animationImages = nil; imgView.image = [UIImage imageNamed:@&quot;shakeHead0&quot;];&#125; 4.SDK downloadBecause the Xunfei face recognition SDK is used in the project, you need to go to the Xunfei open platform to create an application and download the SDK 5.add SDK to the demo Download the demo and import the FBYFaceData folder in the demo into the project. Introduce FBYFaceRecognitionViewController in the project 1#import &quot;FBYFaceRecognitionViewController.h&quot; Add code to the click event of the item recognition button 123456-(void)pushToFaceStreamDetectorVC&#123; FBYFaceRecognitionViewController *faceVC = [[FBYFaceRecognitionViewController alloc]init]; faceVC.faceDelegate = self; [self.navigationController pushViewController:faceVC animated:YES];&#125;","categories":[],"tags":[]},{"title":"WEEKLY REPORTS","slug":"Weekly_Reports","date":"2020-12-13T13:19:00.000Z","updated":"2020-12-13T13:19:06.000Z","comments":true,"path":"Weekly_Reports.html","link":"","permalink":"https://zchen11611.github.io/Weekly_Reports.html","excerpt":"","text":"WEEKLY REPORTSWeek 13-15: continue on my final project: Select retake and repeat steps 5-9, select upload to recall the image data make the data clean Updated my blob site by adding this week report, and fixing the bugs of the final project. Committed the changes to GitHub Week 12: Uploded the code on GitHub, and updated the GitHub link accordingly Collect the data collection of image and install the SDK Attended the online lecture and study Lesson10: Python programming is a common thread through IoT native services, web services, data analysis, and system management IoT communication models Request-response — Django REST framework, Flask, etc. Publish-subscribe — Crossbar.io, Paho, etc. IoT cloud platforms — ThingSpeak, Particle Cloud, etc. Week 11: Watched the recording of lesson’s 9 lecture. Plus, I read and studied lesson 9 slides and lab. About Lesson9: NETCONF protocol and YANG data modeling language provide advanced capabilities of automating IoT system configuration and management Pyang validates and converts YANG modules to various formats such as YIN Netopeer is a set of NETCONF tools built on the Libnetconf library Ncclient is a Python library that facilitates client-side scripting and application development around the NETCONF protocol Cloud native computing, service mesh, and blockchain drive continuous innovation in IoT systems and services Choose a title for my project Week 10: Watched the recording of lesson’s 8 lecture and I studied lesson 8 slides and lab. Week 9: Watched the recording of lesson’s 7 lecture and I studied lesson 7 slides and lab Reviewed the content of the slide. About lesson7: Several PaaS providers offer cloud storage and analytics Google Sheets Requires OAuth 2.0 credentials in JavaScript Object Notation (JSON) and in the same directory as the Python code to open the spreadsheet and append rows Reviewed the material from Github and started doing Lab6 Week 8: Watched the recording of lesson’s 6 lecture and I studied lesson 6 slides and lab I reviewed Python 2 and 3 About lesson6: Alternative Devices A number of IoT development boards support Ethernet, USB development interface, or Linux OS, Form factors vary from 36mm x 20mm to 127mm x 79mm Some include Wi-Fi — few support enterprise Wi-Fi Some have eMMC memory on board Those without HDMI rely on USB development interface Week 6-7: Watched the recording of lesson’s 5 lecture and I studied lesson 5 slides and lab review the report of other students Week 5: I studied Lesson 4 materials and lab, and implemented what I learned about Flask on another cource to present the data on a web page. Week 4: Watched the recording of lesson’s 3 lecture and I studied lesson 3 slides and lab About the Lesson3: Python Python is an interpreted, high-level and general-purpose programming language Data structure, flow control, functions, modules, classes, files, date, time, coordinates, and send email Anaconda To speed up execution, Python interpreter automatically converts source code (.py) once to byte code (.pyc) with the same file name in the same directory Use IDE or text editor to create and edit source code Run secure copy (scp) on laptop to transfer files to and from Raspberry Pi I started doing lab3. I found my Raspberry Pi could use the Python 2.7.16 which had been downloaded in my laptop automatically. Week 3: Reviewed the content of the slide. I spent several days to learn Python About the lesson2L: Raspberry Pi The Raspberry Pi is a low cost, credit-card sized computer that plugs into a computer monitor or TV, and uses a standard keyboard and mouse An SD card inserted into the slot on the board acts as the hard drive for the Raspberry Pi. It is powered by USB and the video output can be hooked up to a traditional RCA TV set, a more modern monitor, or even a TV using the HDMI port Two commonly encountered problems with Raspberry Pi Raspberry Pi serial has two mutually exclusive uses Always double-check GPIO numbering and wiring to avoid damaging Raspberry Pi Week 1-2: Watched the recording of lesson’s lecture and I studied lesson slides(Lesson0-1) I made the GitHub repository for 629 and prepared the basic knowledge framework like Python I finished Lab1and installed Raspberry Pi on VMware Workstation 15. The operating system is Debian 7.x. Then I made some changes in Raspberry Pi Configuration. During Lab02, I learned how to use virtual Raspberry Pi to run circuit, since I used the virtual machine to run Raspberry Pi.","categories":[],"tags":[]}],"categories":[],"tags":[]}